<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2019-10-11T13:55:02+02:00</updated><id>http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/feed.xml</id><title type="html"> </title><subtitle>providing documentation and blog post pages. Stay hungry, stay recording.
</subtitle><entry><title type="html">Interspeech 2019 Voice Conversion Paper Review</title><link href="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/" rel="alternate" type="text/html" title="Interspeech 2019 Voice Conversion Paper Review" /><published>2019-10-10T04:11:09+02:00</published><updated>2019-10-10T04:11:09+02:00</updated><id>http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review</id><content type="html" xml:base="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/">&lt;blockquote&gt;
  &lt;p&gt;For Interspeech 2019, this year there are two sessions about Voice Conversion(VC). In this post, I would mostly review VC-related papers in session &lt;strong&gt;’
Neural Techniques for Voice Conversion and Waveform Generation’&lt;/strong&gt;, which is mainly about speaker information transformation. It is interesting that &lt;strong&gt;StarGAN&lt;/strong&gt; becomes very popular this year. All the three papers about StarGAN tries to improve performace by modifying its architecture or training strategy. Also is &lt;strong&gt;One-shot Learning VC&lt;/strong&gt; (three papers) which convert source speech to arbitrary target speaker with very limited target speaker corpus. One of them uses VAE while other two methods use PPG. There are also three VC works named on Tomoki Toda which all based on VAE framework. (W.I.P)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;ul class=&quot;table-of-content&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#review&quot; id=&quot;markdown-toc-review&quot;&gt;Review&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#non-parallel-voice-conversion-using-weighted-generative-adversarial-networks&quot; id=&quot;markdown-toc-non-parallel-voice-conversion-using-weighted-generative-adversarial-networks&quot;&gt;Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Title&lt;/th&gt;
      &lt;th&gt;Task&lt;/th&gt;
      &lt;th&gt;Framework&lt;/th&gt;
      &lt;th&gt;Author&lt;/th&gt;
      &lt;th&gt;Affiliation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks&lt;/td&gt;
      &lt;td&gt;Non-parallel; many-to-many&lt;/td&gt;
      &lt;td&gt;StarGAN; WORLD&lt;/td&gt;
      &lt;td&gt;Dipjyoti Paul&lt;/td&gt;
      &lt;td&gt;University of Crete, Greece&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization&lt;/td&gt;
      &lt;td&gt;Non-parallel; One-shot&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Ju-chieh Chou; (Hung-yi Lee)&lt;/td&gt;
      &lt;td&gt;National Taiwan University&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;One-Shot Voice Conversion with Global Speaker Embeddings&lt;/td&gt;
      &lt;td&gt;Non-parallel; One-shot&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Hui Lu; (Helen Meng)&lt;/td&gt;
      &lt;td&gt;Tsinghua-CUHK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Non-Parallel Voice Conversion with Cyclic Variational Autoencoder&lt;/td&gt;
      &lt;td&gt;Non-parallel; one-to-one&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Patrick Lumban Tobing; (Tomoki Toda)&lt;/td&gt;
      &lt;td&gt;Nagoya University&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion&lt;/td&gt;
      &lt;td&gt;Non-parallel; many-to-many&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Takuhiro Kaneko&lt;/td&gt;
      &lt;td&gt;NTT&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks&lt;/td&gt;
      &lt;td&gt;Non-parallel; many-to-many&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Shengkui Zhao&lt;/td&gt;
      &lt;td&gt;Alibaba Group(Damo)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams&lt;/td&gt;
      &lt;td&gt;Non-parallel; One-shot&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Seyed Hamidreza Mohammadi&lt;/td&gt;
      &lt;td&gt;ObEN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion&lt;/td&gt;
      &lt;td&gt;Non-parallel; one-to-one&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Wen-Chin Huang; (Tomoki Toda)&lt;/td&gt;
      &lt;td&gt;Nagoya University&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds&lt;/td&gt;
      &lt;td&gt;Environment Robustness&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Yusuke Kurita; (Tomoki Toda)&lt;/td&gt;
      &lt;td&gt;Nagoya University&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams&lt;/td&gt;
      &lt;td&gt;Non-parallel; inf-to-one&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Songxiang Liu; (Lifa Sun); (Helen Meng)&lt;/td&gt;
      &lt;td&gt;CUHK&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion&lt;/td&gt;
      &lt;td&gt;Non-parallel; one-to-one&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Shaojin Ding&lt;/td&gt;
      &lt;td&gt;Texas A&amp;amp;M University&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Semi-Supervised Voice Conversion with Amortized Variational Inference&lt;/td&gt;
      &lt;td&gt;Semi-optimized; one-to-one&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Cory Stephenson&lt;/td&gt;
      &lt;td&gt;Intel AI Lab&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;review&quot;&gt;Review&lt;/h3&gt;
&lt;h4 id=&quot;non-parallel-voice-conversion-using-weighted-generative-adversarial-networks&quot;&gt;Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks&lt;/h4&gt;

&lt;p&gt;This paper modifies loss function in StarGAN. In detail, authors add a weight factor on the adversarial loss when update Generators, which means &lt;script type=&quot;math/tex&quot;&gt;w_iD(G(x_i, c))&lt;/script&gt; . The weight can be considered as the fake confidence for a sample. It reduces the loss of samples which is considered as fake with a high confidence by Discriminator.&lt;/p&gt;</content><author><name>softrime</name></author><summary type="html">For Interspeech 2019, this year there are two sessions about Voice Conversion(VC). In this post, I would mostly review VC-related papers in session ’ Neural Techniques for Voice Conversion and Waveform Generation’, which is mainly about speaker information transformation. It is interesting that StarGAN becomes very popular this year. All the three papers about StarGAN tries to improve performace by modifying its architecture or training strategy. Also is One-shot Learning VC (three papers) which convert source speech to arbitrary target speaker with very limited target speaker corpus. One of them uses VAE while other two methods use PPG. There are also three VC works named on Tomoki Toda which all based on VAE framework. (W.I.P)</summary></entry></feed>