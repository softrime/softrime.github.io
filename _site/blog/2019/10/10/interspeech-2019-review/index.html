<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <link rel="stylesheet" href="/home/softrime/Work/Blog/softrime.github.io/css/main.css">
    <link rel="stylesheet" href="/home/softrime/Work/Blog/softrime.github.io/css/font-awesome.min.css">

    <link rel="shortcut icon" href="/home/softrime/Work/Blog/softrime.github.io/favicon.ico?1">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interspeech 2019 Voice Conversion Paper Review</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Interspeech 2019 Voice Conversion Paper Review" />
<meta name="author" content="softrime" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="For Interspeech 2019, this year there are two sessions about Voice Conversion(VC). In this post, I would mostly review VC-related papers in session ’ Neural Techniques for Voice Conversion and Waveform Generation’, which is mainly about speaker information transformation. It is interesting that StarGAN becomes very popular this year. All the three papers about StarGAN tries to improve performace by modifying its architecture or training strategy. Also is One-shot Learning VC (three papers) which convert source speech to arbitrary target speaker with very limited target speaker corpus. One of them uses VAE while other two methods use PPG. There are also three VC works named on Tomoki Toda which all based on VAE framework. (W.I.P)" />
<meta property="og:description" content="For Interspeech 2019, this year there are two sessions about Voice Conversion(VC). In this post, I would mostly review VC-related papers in session ’ Neural Techniques for Voice Conversion and Waveform Generation’, which is mainly about speaker information transformation. It is interesting that StarGAN becomes very popular this year. All the three papers about StarGAN tries to improve performace by modifying its architecture or training strategy. Also is One-shot Learning VC (three papers) which convert source speech to arbitrary target speaker with very limited target speaker corpus. One of them uses VAE while other two methods use PPG. There are also three VC works named on Tomoki Toda which all based on VAE framework. (W.I.P)" />
<link rel="canonical" href="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/" />
<meta property="og:url" content="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-10-10T04:11:09+02:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/"},"@type":"BlogPosting","url":"http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/","author":{"@type":"Person","name":"softrime"},"headline":"Interspeech 2019 Voice Conversion Paper Review","dateModified":"2019-10-10T04:11:09+02:00","datePublished":"2019-10-10T04:11:09+02:00","description":"For Interspeech 2019, this year there are two sessions about Voice Conversion(VC). In this post, I would mostly review VC-related papers in session ’ Neural Techniques for Voice Conversion and Waveform Generation’, which is mainly about speaker information transformation. It is interesting that StarGAN becomes very popular this year. All the three papers about StarGAN tries to improve performace by modifying its architecture or training strategy. Also is One-shot Learning VC (three papers) which convert source speech to arbitrary target speaker with very limited target speaker corpus. One of them uses VAE while other two methods use PPG. There are also three VC works named on Tomoki Toda which all based on VAE framework. (W.I.P)","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="canonical" href="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/">
    <link rel="alternate" type="application/rss+xml" title=" " href="http://localhost:4000/home/softrime/Work/Blog/softrime.github.io/feed.xml" />
</head>


<body>

    <nav class="navbar navbar-default navbar-fixed-top">
    <div class="container navbar-container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
            <a class="navbar-brand" href="/home/softrime/Work/Blog/softrime.github.io/">
                <span><img src="/home/softrime/Work/Blog/softrime.github.io/img/logonav.png" alt="Logo"></span>  
            </a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav">
                <li  class="active" ><a href="/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/">Blog</a></li>
            </ul>
            <div class="navbar-right">
                <form class="navbar-form navbar-left">
                    <div class="form-group has-feedback">
                        <input id="search-box" type="search" class="form-control" placeholder="Search...">
                        <i class="fa fa-search form-control-feedback"></i>
                    </div>
                </form>
            </div>
        </div>
    </div>
</nav>


    <div class="page-content">
        <div class="wrapper">
            <div class="container">
    <div class="row">

        <div class="col-md-4">
            <div class="well">
                <h4>RECENT POSTS</h4>
                <ul class="list-unstyled post-list-container">
                    
                    <li><a href="/home/softrime/Work/Blog/softrime.github.io/blog/2019/10/10/interspeech-2019-review/"  class="active" >Interspeech 2019 Voice Conversion Paper Review</a></li>
                    
                    <li><a href="/home/softrime/Work/Blog/softrime.github.io/allposts">All posts ...</a></li>
                </ul>
            </div>
        </div>

        <div class="col-md-8">
            <h1 style="font-size:40px">Interspeech 2019 Voice Conversion Paper Review</h1>
            <p>Oct 10, 2019 • softrime</p>
            <div  id="markdown-content-container"><blockquote>
  <p>For Interspeech 2019, this year there are two sessions about Voice Conversion(VC). In this post, I would mostly review VC-related papers in session <strong>’
Neural Techniques for Voice Conversion and Waveform Generation’</strong>, which is mainly about speaker information transformation. It is interesting that <strong>StarGAN</strong> becomes very popular this year. All the three papers about StarGAN tries to improve performace by modifying its architecture or training strategy. Also is <strong>One-shot Learning VC</strong> (three papers) which convert source speech to arbitrary target speaker with very limited target speaker corpus. One of them uses VAE while other two methods use PPG. There are also three VC works named on Tomoki Toda which all based on VAE framework. (W.I.P)</p>
</blockquote>

<!--more-->

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#review" id="markdown-toc-review">Review</a>    <ul>
      <li><a href="#non-parallel-voice-conversion-using-weighted-generative-adversarial-networks" id="markdown-toc-non-parallel-voice-conversion-using-weighted-generative-adversarial-networks">Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks</a></li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="summary">Summary</h3>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Title</th>
      <th>Task</th>
      <th>Framework</th>
      <th>Author</th>
      <th>Affiliation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks</td>
      <td>Non-parallel; many-to-many</td>
      <td>StarGAN; WORLD</td>
      <td>Dipjyoti Paul</td>
      <td>University of Crete, Greece</td>
    </tr>
    <tr>
      <td>2</td>
      <td>One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</td>
      <td>Non-parallel; One-shot</td>
      <td> </td>
      <td>Ju-chieh Chou; (Hung-yi Lee)</td>
      <td>National Taiwan University</td>
    </tr>
    <tr>
      <td>3</td>
      <td>One-Shot Voice Conversion with Global Speaker Embeddings</td>
      <td>Non-parallel; One-shot</td>
      <td> </td>
      <td>Hui Lu; (Helen Meng)</td>
      <td>Tsinghua-CUHK</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Non-Parallel Voice Conversion with Cyclic Variational Autoencoder</td>
      <td>Non-parallel; one-to-one</td>
      <td> </td>
      <td>Patrick Lumban Tobing; (Tomoki Toda)</td>
      <td>Nagoya University</td>
    </tr>
    <tr>
      <td>5</td>
      <td>StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion</td>
      <td>Non-parallel; many-to-many</td>
      <td> </td>
      <td>Takuhiro Kaneko</td>
      <td>NTT</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks</td>
      <td>Non-parallel; many-to-many</td>
      <td> </td>
      <td>Shengkui Zhao</td>
      <td>Alibaba Group(Damo)</td>
    </tr>
    <tr>
      <td>7</td>
      <td>One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams</td>
      <td>Non-parallel; One-shot</td>
      <td> </td>
      <td>Seyed Hamidreza Mohammadi</td>
      <td>ObEN</td>
    </tr>
    <tr>
      <td>8</td>
      <td>Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion</td>
      <td>Non-parallel; one-to-one</td>
      <td> </td>
      <td>Wen-Chin Huang; (Tomoki Toda)</td>
      <td>Nagoya University</td>
    </tr>
    <tr>
      <td>9</td>
      <td>Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds</td>
      <td>Environment Robustness</td>
      <td> </td>
      <td>Yusuke Kurita; (Tomoki Toda)</td>
      <td>Nagoya University</td>
    </tr>
    <tr>
      <td>10</td>
      <td>Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams</td>
      <td>Non-parallel; inf-to-one</td>
      <td> </td>
      <td>Songxiang Liu; (Lifa Sun); (Helen Meng)</td>
      <td>CUHK</td>
    </tr>
    <tr>
      <td>11</td>
      <td>Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion</td>
      <td>Non-parallel; one-to-one</td>
      <td> </td>
      <td>Shaojin Ding</td>
      <td>Texas A&amp;M University</td>
    </tr>
    <tr>
      <td>12</td>
      <td>Semi-Supervised Voice Conversion with Amortized Variational Inference</td>
      <td>Semi-optimized; one-to-one</td>
      <td> </td>
      <td>Cory Stephenson</td>
      <td>Intel AI Lab</td>
    </tr>
  </tbody>
</table>

<h3 id="review">Review</h3>
<h4 id="non-parallel-voice-conversion-using-weighted-generative-adversarial-networks">Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks</h4>

<p>This paper modifies loss function in StarGAN. In detail, authors add a weight factor on the adversarial loss when update Generators, which means <script type="math/tex">w_iD(G(x_i, c))</script> . The weight can be considered as the fake confidence for a sample. It reduces the loss of samples which is considered as fake with a high confidence by Discriminator.</p>

</div>
            <hr>
            <ul class="pager">
                 
            </ul>
        </div>

    </div>
</div>

        </div>
    </div>

    <footer class="footer">
    <div class="container">
        <p class="text-center">
            SOFTRIME &copy; 2019 |
            Built by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> and <a href="https://github.com/aksakalli/jekyll-doc-theme">Jekyll Doc Theme</a> | 
	    View <a href="https://github.com/softrime/softrime.github.io" target="_blank">this</a> on Github
        </p>
        <!-- <p class="text-muted">Place sticky footer content here.</p> -->
    </div>
</footer>

    <script>
  var baseurl = '/home/softrime/Work/Blog/softrime.github.io'
</script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="/home/softrime/Work/Blog/softrime.github.io/js/bootstrap.min.js "></script>
<script src="/home/softrime/Work/Blog/softrime.github.io/js/typeahead.bundle.min.js "></script>

<script src="/home/softrime/Work/Blog/softrime.github.io/js/main.js "></script>

</body>

</html>
